{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation: Fake News Detector Agent\n",
    "\n",
    "This notebook evaluates different LLM models across the agent's nodes:\n",
    "- **Entry**: Relevance analysis\n",
    "- **Planner**: Plan generation\n",
    "- **Researcher**: Query generation\n",
    "- **Analyst**: Final analysis and scoring\n",
    "\n",
    "## Baseline: GPT-4\n",
    "## Test Models: llama3.1:8b, qwen2.5:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "from src.graphs.v1 import graph\n",
    "from src.models.context import ModelsRegistry, ModelConfig\n",
    "from tavily import TavilyClient\n",
    "from src.utils.observability import (\n",
    "    UsageMetadataCallbackHandler,\n",
    "    set_callback_handler,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Tavily client\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test posts - mix of relevant and irrelevant content\n",
    "TEST_POSTS = [\n",
    "    {\n",
    "        \"id\": \"irrelevant_1\",\n",
    "        \"text\": \"Bom dia!\",\n",
    "        \"expected_relevant\": False,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"fake_news_1\",\n",
    "        \"text\": \"Cloroquina cura COVID\",\n",
    "        \"expected_relevant\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"political_1\",\n",
    "        \"text\": \"Gastar dinheiro público com campanhas de incentivo ao aborto, ideologia de gênero ou troca de sexo de crianças? NÃO! Não vamos permitir!\",\n",
    "        \"expected_relevant\": True,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Baseline model (GPT-4)\n",
    "BASELINE_MODEL = \"gpt-4\"\n",
    "\n",
    "# Ollama configuration for local models\n",
    "OLLAMA_CONFIG = {\n",
    "    \"model_provider\": \"ollama\",\n",
    "    \"base_url\": \"http://localhost:11434\",\n",
    "}\n",
    "\n",
    "# Models to test\n",
    "TEST_MODELS = [\n",
    "    \"llama3.1:8b\",\n",
    "    \"qwen2.5:1.5b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_registry() -> ModelsRegistry:\n",
    "    \"\"\"Create registry with baseline model (GPT-4) for all nodes.\"\"\"\n",
    "    return ModelsRegistry(\n",
    "        entry=BASELINE_MODEL,\n",
    "        planner=BASELINE_MODEL,\n",
    "        researcher=BASELINE_MODEL,\n",
    "        analyst=BASELINE_MODEL,\n",
    "        default_temperature=0.0,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_test_registry(node_name: str, test_model: str) -> ModelsRegistry:\n",
    "    \"\"\"\n",
    "    Create registry with test model for a specific node, baseline for others.\n",
    "    \n",
    "    Args:\n",
    "        node_name: Node to use test model (entry, planner, researcher, analyst)\n",
    "        test_model: Model name to test (e.g., llama3.1:8b)\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"entry\": BASELINE_MODEL,\n",
    "        \"planner\": BASELINE_MODEL,\n",
    "        \"researcher\": BASELINE_MODEL,\n",
    "        \"analyst\": BASELINE_MODEL,\n",
    "        \"default_temperature\": 0.0,\n",
    "    }\n",
    "    \n",
    "    # Replace the specific node with test model config\n",
    "    config[node_name] = ModelConfig(\n",
    "        model=test_model,\n",
    "        temperature=0.0,\n",
    "        **OLLAMA_CONFIG,\n",
    "    )\n",
    "    \n",
    "    return ModelsRegistry(**config)\n",
    "\n",
    "\n",
    "def run_evaluation(registry: ModelsRegistry, post: str) -> Dict[str, Any]:\n",
    "    \"\"\"Run the graph with a specific registry and return results.\"\"\"\n",
    "    # Create fresh callback handler for each run\n",
    "    callback = UsageMetadataCallbackHandler()\n",
    "    set_callback_handler(callback)\n",
    "    \n",
    "    config = {\n",
    "        \"configurable\": {\"thread_id\": str(datetime.now().timestamp())},\n",
    "        \"callbacks\": [callback],\n",
    "    }\n",
    "    runtime_context = {\"models_registry\": registry, \"tavily\": tavily}\n",
    "    \n",
    "    initial_state = {\n",
    "        \"post\": post,\n",
    "        \"max_revisions\": 3,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        resp = graph.invoke(initial_state, context=runtime_context, config=config)\n",
    "        \n",
    "        # Extract results\n",
    "        result = {\n",
    "            \"success\": True,\n",
    "            \"relevance_analysis\": resp[\"relevance_analysis\"].model_dump() if hasattr(resp[\"relevance_analysis\"], \"model_dump\") else resp[\"relevance_analysis\"],\n",
    "            \"metrics\": {k: v.model_dump() for k, v in resp[\"metrics\"].items()},\n",
    "        }\n",
    "        \n",
    "        # Add response if relevant\n",
    "        if result[\"relevance_analysis\"].get(\"relevant\", False):\n",
    "            result[\"plan\"] = resp.get(\"plan\", \"\")\n",
    "            result[\"response\"] = resp[\"response\"].model_dump() if hasattr(resp[\"response\"], \"model_dump\") else resp[\"response\"]\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run Baseline Evaluation (GPT-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline evaluation with GPT-4...\n",
      "\n",
      "Processing: irrelevant_1\n",
      "  Post: Bom dia!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\repos\\pessoal\\fake-news-detector-agent\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:2069: UserWarning: Cannot use method='json_schema' with model gpt-4 since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[entry] Execution time: 18.83s | Base model: gpt-4 | Tokens: 615 (585 prompt + 30 completion)\n",
      "  Relevant: False\n",
      "\n",
      "Processing: fake_news_1\n",
      "  Post: Cloroquina cura COVID\n",
      "[entry] Execution time: 2.41s | Base model: gpt-4 | Tokens: 615 (587 prompt + 28 completion)\n",
      "[planner] Execution time: 11.50s | Base model: gpt-4 | Tokens: 602 (226 prompt + 376 completion)\n",
      "[researcher] Execution time: 16.36s | Base model: gpt-4 | Tokens: 732 (612 prompt + 120 completion)\n",
      "[analyst] Execution time: 4.35s | Base model: gpt-4 | Tokens: 1678 (1531 prompt + 147 completion)\n",
      "  Relevant: True\n",
      "  Score: 1.0\n",
      "\n",
      "Processing: political_1\n",
      "  Post: Gastar dinheiro público com campanhas de incentivo...\n",
      "[entry] Execution time: 2.39s | Base model: gpt-4 | Tokens: 656 (620 prompt + 36 completion)\n",
      "[planner] Execution time: 14.81s | Base model: gpt-4 | Tokens: 648 (259 prompt + 389 completion)\n",
      "[researcher] Execution time: 7.57s | Base model: gpt-4 | Tokens: 686 (625 prompt + 61 completion)\n",
      "[analyst] Execution time: 5.36s | Base model: gpt-4 | Tokens: 1153 (1031 prompt + 122 completion)\n",
      "  Relevant: True\n",
      "  Score: 0.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Running baseline evaluation with GPT-4...\\n\")\n",
    "\n",
    "baseline_results = {}\n",
    "baseline_registry = create_baseline_registry()\n",
    "\n",
    "for post_data in TEST_POSTS:\n",
    "    post_id = post_data[\"id\"]\n",
    "    post_text = post_data[\"text\"]\n",
    "    \n",
    "    print(f\"Processing: {post_id}\")\n",
    "    print(f\"  Post: {post_text[:50]}...\" if len(post_text) > 50 else f\"  Post: {post_text}\")\n",
    "    \n",
    "    result = run_evaluation(baseline_registry, post_text)\n",
    "    baseline_results[post_id] = result\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(f\"  Relevant: {result['relevance_analysis']['relevant']}\")\n",
    "        if result['relevance_analysis']['relevant']:\n",
    "            print(f\"  Score: {result['response']['score']}\")\n",
    "    else:\n",
    "        print(f\"  Error: {result['error']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Different Models per Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing model: llama3.1:8b\n",
      "============================================================\n",
      "\n",
      "--- Node: entry ---\n",
      "  Processing: irrelevant_1... [entry] Execution time: 56.70s | Base model: llama3.1:8b | Tokens: 581 (548 prompt + 33 completion)\n",
      "OK (relevant=False)\n",
      "  Processing: fake_news_1... [entry] Execution time: 7.44s | Base model: llama3.1:8b | Tokens: 577 (550 prompt + 27 completion)\n",
      "[planner] Execution time: 11.73s | Base model: gpt-4 | Tokens: 613 (226 prompt + 387 completion)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\repos\\pessoal\\fake-news-detector-agent\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:2069: UserWarning: Cannot use method='json_schema' with model gpt-4 since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[researcher] Execution time: 8.28s | Base model: gpt-4 | Tokens: 721 (623 prompt + 98 completion)\n",
      "[analyst] Execution time: 5.58s | Base model: gpt-4 | Tokens: 1498 (1354 prompt + 144 completion)\n",
      "OK (relevant=True)\n",
      "  Processing: political_1... [entry] Execution time: 11.74s | Base model: llama3.1:8b | Tokens: 612 (583 prompt + 29 completion)\n",
      "OK (relevant=False)\n",
      "\n",
      "--- Node: planner ---\n",
      "  Processing: irrelevant_1... [entry] Execution time: 1.83s | Base model: gpt-4 | Tokens: 615 (585 prompt + 30 completion)\n",
      "OK (relevant=False)\n",
      "  Processing: fake_news_1... [entry] Execution time: 1.44s | Base model: gpt-4 | Tokens: 615 (587 prompt + 28 completion)\n",
      "[planner] Execution time: 154.78s | Base model: llama3.1:8b | Tokens: 814 (230 prompt + 584 completion)\n",
      "[researcher] Execution time: 8.42s | Base model: gpt-4 | Tokens: 885 (820 prompt + 65 completion)\n",
      "[analyst] Execution time: 6.58s | Base model: gpt-4 | Tokens: 1629 (1525 prompt + 104 completion)\n",
      "OK (relevant=True)\n",
      "  Processing: political_1... [entry] Execution time: 1.54s | Base model: gpt-4 | Tokens: 657 (620 prompt + 37 completion)\n",
      "[planner] Execution time: 115.57s | Base model: llama3.1:8b | Tokens: 720 (263 prompt + 457 completion)\n",
      "[researcher] Execution time: 9.64s | Base model: gpt-4 | Tokens: 786 (694 prompt + 92 completion)\n",
      "[analyst] Execution time: 3.78s | Base model: gpt-4 | Tokens: 2387 (2260 prompt + 127 completion)\n",
      "OK (relevant=True)\n",
      "\n",
      "--- Node: researcher ---\n",
      "  Processing: irrelevant_1... [entry] Execution time: 1.26s | Base model: gpt-4 | Tokens: 615 (585 prompt + 30 completion)\n",
      "OK (relevant=False)\n",
      "  Processing: fake_news_1... [entry] Execution time: 1.31s | Base model: gpt-4 | Tokens: 615 (587 prompt + 28 completion)\n",
      "[planner] Execution time: 7.84s | Base model: gpt-4 | Tokens: 517 (226 prompt + 291 completion)\n",
      "[researcher] Execution time: 47.03s | Base model: llama3.1:8b | Tokens: 598 (499 prompt + 99 completion)\n",
      "[analyst] Execution time: 6.28s | Base model: gpt-4 | Tokens: 2041 (1906 prompt + 135 completion)\n",
      "OK (relevant=True)\n",
      "  Processing: political_1... [entry] Execution time: 2.60s | Base model: gpt-4 | Tokens: 655 (620 prompt + 35 completion)\n",
      "[planner] Execution time: 11.28s | Base model: gpt-4 | Tokens: 648 (259 prompt + 389 completion)\n",
      "[researcher] Execution time: 30.47s | Base model: llama3.1:8b | Tokens: 660 (597 prompt + 63 completion)\n",
      "[analyst] Execution time: 5.00s | Base model: gpt-4 | Tokens: 1214 (1058 prompt + 156 completion)\n",
      "OK (relevant=True)\n",
      "\n",
      "--- Node: analyst ---\n",
      "  Processing: irrelevant_1... [entry] Execution time: 1.23s | Base model: gpt-4 | Tokens: 615 (585 prompt + 30 completion)\n",
      "OK (relevant=False)\n",
      "  Processing: fake_news_1... [entry] Execution time: 1.67s | Base model: gpt-4 | Tokens: 615 (587 prompt + 28 completion)\n",
      "[planner] Execution time: 10.56s | Base model: gpt-4 | Tokens: 621 (226 prompt + 395 completion)\n",
      "[researcher] Execution time: 8.92s | Base model: gpt-4 | Tokens: 704 (631 prompt + 73 completion)\n",
      "[analyst] Execution time: 66.80s | Base model: llama3.1:8b | Tokens: 1854 (1747 prompt + 107 completion)\n",
      "OK (relevant=True)\n",
      "  Processing: political_1... [entry] Execution time: 1.51s | Base model: gpt-4 | Tokens: 656 (620 prompt + 36 completion)\n",
      "[planner] Execution time: 13.26s | Base model: gpt-4 | Tokens: 698 (259 prompt + 439 completion)\n",
      "[researcher] Execution time: 4.80s | Base model: gpt-4 | Tokens: 736 (675 prompt + 61 completion)\n",
      "[analyst] Execution time: 45.84s | Base model: llama3.1:8b | Tokens: 1162 (1049 prompt + 113 completion)\n",
      "OK (relevant=True)\n",
      "\n",
      "============================================================\n",
      "Testing model: qwen2.5:1.5b\n",
      "============================================================\n",
      "\n",
      "--- Node: entry ---\n",
      "  Processing: irrelevant_1... [entry] Execution time: 13.59s | Base model: qwen2.5:1.5b | Tokens: 577 (546 prompt + 31 completion)\n",
      "OK (relevant=False)\n",
      "  Processing: fake_news_1... [entry] Execution time: 1.17s | Base model: qwen2.5:1.5b | Tokens: 591 (548 prompt + 43 completion)\n",
      "[planner] Execution time: 11.95s | Base model: gpt-4 | Tokens: 535 (226 prompt + 309 completion)\n",
      "[researcher] Execution time: 7.78s | Base model: gpt-4 | Tokens: 618 (545 prompt + 73 completion)\n",
      "[analyst] Execution time: 5.70s | Base model: gpt-4 | Tokens: 1346 (1220 prompt + 126 completion)\n",
      "OK (relevant=True)\n",
      "  Processing: political_1... [entry] Execution time: 3.14s | Base model: qwen2.5:1.5b | Tokens: 615 (580 prompt + 35 completion)\n",
      "[planner] Execution time: 9.17s | Base model: gpt-4 | Tokens: 641 (259 prompt + 382 completion)\n",
      "[researcher] Execution time: 5.07s | Base model: gpt-4 | Tokens: 679 (618 prompt + 61 completion)\n",
      "[analyst] Execution time: 3.15s | Base model: gpt-4 | Tokens: 1125 (1024 prompt + 101 completion)\n",
      "OK (relevant=True)\n",
      "\n",
      "--- Node: planner ---\n",
      "  Processing: irrelevant_1... [entry] Execution time: 1.90s | Base model: gpt-4 | Tokens: 615 (585 prompt + 30 completion)\n",
      "OK (relevant=False)\n",
      "  Processing: fake_news_1... [entry] Execution time: 1.31s | Base model: gpt-4 | Tokens: 615 (587 prompt + 28 completion)\n",
      "[planner] Execution time: 30.18s | Base model: qwen2.5:1.5b | Tokens: 1352 (224 prompt + 1128 completion)\n",
      "[researcher] Execution time: 15.44s | Base model: gpt-4 | Tokens: 1425 (1366 prompt + 59 completion)\n",
      "[analyst] Execution time: 5.19s | Base model: gpt-4 | Tokens: 1786 (1681 prompt + 105 completion)\n",
      "OK (relevant=True)\n",
      "  Processing: political_1... [entry] Execution time: 2.00s | Base model: gpt-4 | Tokens: 655 (620 prompt + 35 completion)\n",
      "[planner] Execution time: 14.64s | Base model: qwen2.5:1.5b | Tokens: 796 (256 prompt + 540 completion)\n",
      "[researcher] Execution time: 27.41s | Base model: gpt-4 | Tokens: 849 (791 prompt + 58 completion)\n",
      "[analyst] Execution time: 4.47s | Base model: gpt-4 | Tokens: 1345 (1200 prompt + 145 completion)\n",
      "OK (relevant=True)\n",
      "\n",
      "--- Node: researcher ---\n",
      "  Processing: irrelevant_1... [entry] Execution time: 1.35s | Base model: gpt-4 | Tokens: 615 (585 prompt + 30 completion)\n",
      "OK (relevant=False)\n",
      "  Processing: fake_news_1... [entry] Execution time: 1.36s | Base model: gpt-4 | Tokens: 615 (587 prompt + 28 completion)\n",
      "[planner] Execution time: 9.01s | Base model: gpt-4 | Tokens: 574 (226 prompt + 348 completion)\n",
      "[researcher] Execution time: 15.18s | Base model: qwen2.5:1.5b | Tokens: 674 (557 prompt + 117 completion)\n",
      "[analyst] Execution time: 4.67s | Base model: gpt-4 | Tokens: 1087 (972 prompt + 115 completion)\n",
      "OK (relevant=True)\n",
      "  Processing: political_1... [entry] Execution time: 1.63s | Base model: gpt-4 | Tokens: 655 (620 prompt + 35 completion)\n",
      "[planner] Execution time: 15.44s | Base model: gpt-4 | Tokens: 667 (259 prompt + 408 completion)\n",
      "[researcher] Execution time: 19.44s | Base model: qwen2.5:1.5b | Tokens: 731 (611 prompt + 120 completion)\n",
      "[analyst] Execution time: 3.80s | Base model: gpt-4 | Tokens: 1391 (1260 prompt + 131 completion)\n",
      "OK (relevant=True)\n",
      "\n",
      "--- Node: analyst ---\n",
      "  Processing: irrelevant_1... [entry] Execution time: 1.60s | Base model: gpt-4 | Tokens: 615 (585 prompt + 30 completion)\n",
      "OK (relevant=False)\n",
      "  Processing: fake_news_1... [entry] Execution time: 1.97s | Base model: gpt-4 | Tokens: 615 (587 prompt + 28 completion)\n",
      "[planner] Execution time: 18.49s | Base model: gpt-4 | Tokens: 665 (226 prompt + 439 completion)\n",
      "[researcher] Execution time: 8.02s | Base model: gpt-4 | Tokens: 745 (675 prompt + 70 completion)\n",
      "[analyst] Execution time: 12.99s | Base model: qwen2.5:1.5b | Tokens: 1962 (1807 prompt + 155 completion)\n",
      "OK (relevant=True)\n",
      "  Processing: political_1... [entry] Execution time: 1.66s | Base model: gpt-4 | Tokens: 657 (620 prompt + 37 completion)\n",
      "[planner] Execution time: 14.87s | Base model: gpt-4 | Tokens: 701 (259 prompt + 442 completion)\n",
      "[researcher] Execution time: 16.25s | Base model: gpt-4 | Tokens: 815 (678 prompt + 137 completion)\n",
      "[analyst] Execution time: 10.26s | Base model: qwen2.5:1.5b | Tokens: 1886 (1769 prompt + 117 completion)\n",
      "OK (relevant=True)\n"
     ]
    }
   ],
   "source": [
    "# Store all test results\n",
    "test_results = {}\n",
    "nodes = [\"entry\", \"planner\", \"researcher\", \"analyst\"]\n",
    "\n",
    "for test_model in TEST_MODELS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing model: {test_model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    test_results[test_model] = {}\n",
    "    \n",
    "    for node in nodes:\n",
    "        print(f\"\\n--- Node: {node} ---\")\n",
    "        test_results[test_model][node] = {}\n",
    "        \n",
    "        registry = create_test_registry(node, test_model)\n",
    "        \n",
    "        for post_data in TEST_POSTS:\n",
    "            post_id = post_data[\"id\"]\n",
    "            post_text = post_data[\"text\"]\n",
    "            \n",
    "            print(f\"  Processing: {post_id}...\", end=\" \")\n",
    "            \n",
    "            result = run_evaluation(registry, post_text)\n",
    "            test_results[test_model][node][post_id] = result\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                print(f\"OK (relevant={result['relevance_analysis']['relevant']})\")\n",
    "            else:\n",
    "                print(f\"FAILED: {result['error'][:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Results with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_relevance(baseline: Dict, test: Dict) -> Dict:\n",
    "    \"\"\"Compare relevance analysis between baseline and test.\"\"\"\n",
    "    if not baseline[\"success\"] or not test[\"success\"]:\n",
    "        return {\"match\": False, \"reason\": \"One or both failed\"}\n",
    "    \n",
    "    b_rel = baseline[\"relevance_analysis\"][\"relevant\"]\n",
    "    t_rel = test[\"relevance_analysis\"][\"relevant\"]\n",
    "    \n",
    "    return {\n",
    "        \"match\": b_rel == t_rel,\n",
    "        \"baseline_relevant\": b_rel,\n",
    "        \"test_relevant\": t_rel,\n",
    "        \"baseline_reasoning\": baseline[\"relevance_analysis\"].get(\"reasoning\", \"\")[:100],\n",
    "        \"test_reasoning\": test[\"relevance_analysis\"].get(\"reasoning\", \"\")[:100],\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_scores(baseline: Dict, test: Dict) -> Dict:\n",
    "    \"\"\"Compare final scores between baseline and test.\"\"\"\n",
    "    if not baseline[\"success\"] or not test[\"success\"]:\n",
    "        return {\"comparable\": False, \"reason\": \"One or both failed\"}\n",
    "    \n",
    "    b_rel = baseline[\"relevance_analysis\"][\"relevant\"]\n",
    "    t_rel = test[\"relevance_analysis\"][\"relevant\"]\n",
    "    \n",
    "    if not b_rel or not t_rel:\n",
    "        return {\"comparable\": False, \"reason\": \"One or both not relevant\"}\n",
    "    \n",
    "    b_score = baseline[\"response\"][\"score\"]\n",
    "    t_score = test[\"response\"][\"score\"]\n",
    "    \n",
    "    return {\n",
    "        \"comparable\": True,\n",
    "        \"baseline_score\": b_score,\n",
    "        \"test_score\": t_score,\n",
    "        \"difference\": t_score - b_score,\n",
    "        \"baseline_justification\": baseline[\"response\"].get(\"justification\", \"\")[:200],\n",
    "        \"test_justification\": test[\"response\"].get(\"justification\", \"\")[:200],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>node</th>\n",
       "      <th>post_id</th>\n",
       "      <th>success</th>\n",
       "      <th>relevance_match</th>\n",
       "      <th>baseline_relevant</th>\n",
       "      <th>test_relevant</th>\n",
       "      <th>baseline_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>score_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>entry</td>\n",
       "      <td>irrelevant_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>entry</td>\n",
       "      <td>fake_news_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>entry</td>\n",
       "      <td>political_1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>planner</td>\n",
       "      <td>irrelevant_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>planner</td>\n",
       "      <td>fake_news_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>planner</td>\n",
       "      <td>political_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>researcher</td>\n",
       "      <td>irrelevant_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>researcher</td>\n",
       "      <td>fake_news_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>researcher</td>\n",
       "      <td>political_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>analyst</td>\n",
       "      <td>irrelevant_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>analyst</td>\n",
       "      <td>fake_news_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama3.1:8b</td>\n",
       "      <td>analyst</td>\n",
       "      <td>political_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>entry</td>\n",
       "      <td>irrelevant_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>entry</td>\n",
       "      <td>fake_news_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>entry</td>\n",
       "      <td>political_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>planner</td>\n",
       "      <td>irrelevant_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>planner</td>\n",
       "      <td>fake_news_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>planner</td>\n",
       "      <td>political_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>researcher</td>\n",
       "      <td>irrelevant_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>researcher</td>\n",
       "      <td>fake_news_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>researcher</td>\n",
       "      <td>political_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>analyst</td>\n",
       "      <td>irrelevant_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>analyst</td>\n",
       "      <td>fake_news_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>analyst</td>\n",
       "      <td>political_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model        node       post_id  success  relevance_match  \\\n",
       "0    llama3.1:8b       entry  irrelevant_1     True             True   \n",
       "1    llama3.1:8b       entry   fake_news_1     True             True   \n",
       "2    llama3.1:8b       entry   political_1     True            False   \n",
       "3    llama3.1:8b     planner  irrelevant_1     True             True   \n",
       "4    llama3.1:8b     planner   fake_news_1     True             True   \n",
       "5    llama3.1:8b     planner   political_1     True             True   \n",
       "6    llama3.1:8b  researcher  irrelevant_1     True             True   \n",
       "7    llama3.1:8b  researcher   fake_news_1     True             True   \n",
       "8    llama3.1:8b  researcher   political_1     True             True   \n",
       "9    llama3.1:8b     analyst  irrelevant_1     True             True   \n",
       "10   llama3.1:8b     analyst   fake_news_1     True             True   \n",
       "11   llama3.1:8b     analyst   political_1     True             True   \n",
       "12  qwen2.5:1.5b       entry  irrelevant_1     True             True   \n",
       "13  qwen2.5:1.5b       entry   fake_news_1     True             True   \n",
       "14  qwen2.5:1.5b       entry   political_1     True             True   \n",
       "15  qwen2.5:1.5b     planner  irrelevant_1     True             True   \n",
       "16  qwen2.5:1.5b     planner   fake_news_1     True             True   \n",
       "17  qwen2.5:1.5b     planner   political_1     True             True   \n",
       "18  qwen2.5:1.5b  researcher  irrelevant_1     True             True   \n",
       "19  qwen2.5:1.5b  researcher   fake_news_1     True             True   \n",
       "20  qwen2.5:1.5b  researcher   political_1     True             True   \n",
       "21  qwen2.5:1.5b     analyst  irrelevant_1     True             True   \n",
       "22  qwen2.5:1.5b     analyst   fake_news_1     True             True   \n",
       "23  qwen2.5:1.5b     analyst   political_1     True             True   \n",
       "\n",
       "    baseline_relevant  test_relevant  baseline_score  test_score  score_diff  \n",
       "0               False          False             NaN         NaN         NaN  \n",
       "1                True           True             1.0         0.7        -0.3  \n",
       "2                True          False             NaN         NaN         NaN  \n",
       "3               False          False             NaN         NaN         NaN  \n",
       "4                True           True             1.0         0.9        -0.1  \n",
       "5                True           True             0.9         0.9         0.0  \n",
       "6               False          False             NaN         NaN         NaN  \n",
       "7                True           True             1.0         1.0         0.0  \n",
       "8                True           True             0.9         0.8        -0.1  \n",
       "9               False          False             NaN         NaN         NaN  \n",
       "10               True           True             1.0         1.0         0.0  \n",
       "11               True           True             0.9         0.9         0.0  \n",
       "12              False          False             NaN         NaN         NaN  \n",
       "13               True           True             1.0         0.9        -0.1  \n",
       "14               True           True             0.9         0.9         0.0  \n",
       "15              False          False             NaN         NaN         NaN  \n",
       "16               True           True             1.0         0.9        -0.1  \n",
       "17               True           True             0.9         0.8        -0.1  \n",
       "18              False          False             NaN         NaN         NaN  \n",
       "19               True           True             1.0         1.0         0.0  \n",
       "20               True           True             0.9         1.0         0.1  \n",
       "21              False          False             NaN         NaN         NaN  \n",
       "22               True           True             1.0         0.5        -0.5  \n",
       "23               True           True             0.9         0.9         0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for test_model in TEST_MODELS:\n",
    "    for node in nodes:\n",
    "        for post_data in TEST_POSTS:\n",
    "            post_id = post_data[\"id\"]\n",
    "            \n",
    "            baseline = baseline_results.get(post_id, {})\n",
    "            test = test_results.get(test_model, {}).get(node, {}).get(post_id, {})\n",
    "            \n",
    "            rel_comparison = compare_relevance(baseline, test)\n",
    "            score_comparison = compare_scores(baseline, test)\n",
    "            \n",
    "            row = {\n",
    "                \"model\": test_model,\n",
    "                \"node\": node,\n",
    "                \"post_id\": post_id,\n",
    "                \"success\": test.get(\"success\", False),\n",
    "                \"relevance_match\": rel_comparison.get(\"match\", False),\n",
    "                \"baseline_relevant\": rel_comparison.get(\"baseline_relevant\"),\n",
    "                \"test_relevant\": rel_comparison.get(\"test_relevant\"),\n",
    "            }\n",
    "            \n",
    "            if score_comparison.get(\"comparable\"):\n",
    "                row[\"baseline_score\"] = score_comparison[\"baseline_score\"]\n",
    "                row[\"test_score\"] = score_comparison[\"test_score\"]\n",
    "                row[\"score_diff\"] = score_comparison[\"difference\"]\n",
    "            \n",
    "            comparison_data.append(row)\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metrics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>tested_node</th>\n",
       "      <th>metric_node</th>\n",
       "      <th>post_id</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BASELINE (GPT-4)</td>\n",
       "      <td>all</td>\n",
       "      <td>entry</td>\n",
       "      <td>irrelevant_1</td>\n",
       "      <td>18.826879</td>\n",
       "      <td>585</td>\n",
       "      <td>30</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BASELINE (GPT-4)</td>\n",
       "      <td>all</td>\n",
       "      <td>entry</td>\n",
       "      <td>fake_news_1</td>\n",
       "      <td>2.408844</td>\n",
       "      <td>587</td>\n",
       "      <td>28</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BASELINE (GPT-4)</td>\n",
       "      <td>all</td>\n",
       "      <td>planner</td>\n",
       "      <td>fake_news_1</td>\n",
       "      <td>11.495560</td>\n",
       "      <td>226</td>\n",
       "      <td>376</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BASELINE (GPT-4)</td>\n",
       "      <td>all</td>\n",
       "      <td>researcher</td>\n",
       "      <td>fake_news_1</td>\n",
       "      <td>16.358005</td>\n",
       "      <td>612</td>\n",
       "      <td>120</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BASELINE (GPT-4)</td>\n",
       "      <td>all</td>\n",
       "      <td>analyst</td>\n",
       "      <td>fake_news_1</td>\n",
       "      <td>4.350441</td>\n",
       "      <td>1531</td>\n",
       "      <td>147</td>\n",
       "      <td>1678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>analyst</td>\n",
       "      <td>analyst</td>\n",
       "      <td>fake_news_1</td>\n",
       "      <td>12.992837</td>\n",
       "      <td>1807</td>\n",
       "      <td>155</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>analyst</td>\n",
       "      <td>entry</td>\n",
       "      <td>political_1</td>\n",
       "      <td>1.662532</td>\n",
       "      <td>620</td>\n",
       "      <td>37</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>analyst</td>\n",
       "      <td>planner</td>\n",
       "      <td>political_1</td>\n",
       "      <td>14.870713</td>\n",
       "      <td>259</td>\n",
       "      <td>442</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>analyst</td>\n",
       "      <td>researcher</td>\n",
       "      <td>political_1</td>\n",
       "      <td>16.247149</td>\n",
       "      <td>678</td>\n",
       "      <td>137</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>qwen2.5:1.5b</td>\n",
       "      <td>analyst</td>\n",
       "      <td>analyst</td>\n",
       "      <td>political_1</td>\n",
       "      <td>10.259957</td>\n",
       "      <td>1769</td>\n",
       "      <td>117</td>\n",
       "      <td>1886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               model tested_node metric_node       post_id  execution_time  \\\n",
       "0   BASELINE (GPT-4)         all       entry  irrelevant_1       18.826879   \n",
       "1   BASELINE (GPT-4)         all       entry   fake_news_1        2.408844   \n",
       "2   BASELINE (GPT-4)         all     planner   fake_news_1       11.495560   \n",
       "3   BASELINE (GPT-4)         all  researcher   fake_news_1       16.358005   \n",
       "4   BASELINE (GPT-4)         all     analyst   fake_news_1        4.350441   \n",
       "..               ...         ...         ...           ...             ...   \n",
       "73      qwen2.5:1.5b     analyst     analyst   fake_news_1       12.992837   \n",
       "74      qwen2.5:1.5b     analyst       entry   political_1        1.662532   \n",
       "75      qwen2.5:1.5b     analyst     planner   political_1       14.870713   \n",
       "76      qwen2.5:1.5b     analyst  researcher   political_1       16.247149   \n",
       "77      qwen2.5:1.5b     analyst     analyst   political_1       10.259957   \n",
       "\n",
       "    prompt_tokens  completion_tokens  total_tokens  \n",
       "0             585                 30           615  \n",
       "1             587                 28           615  \n",
       "2             226                376           602  \n",
       "3             612                120           732  \n",
       "4            1531                147          1678  \n",
       "..            ...                ...           ...  \n",
       "73           1807                155          1962  \n",
       "74            620                 37           657  \n",
       "75            259                442           701  \n",
       "76            678                137           815  \n",
       "77           1769                117          1886  \n",
       "\n",
       "[78 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract metrics for comparison\n",
    "metrics_data = []\n",
    "\n",
    "# Baseline metrics\n",
    "for post_id, result in baseline_results.items():\n",
    "    if result[\"success\"]:\n",
    "        for node_name, node_metrics in result[\"metrics\"].items():\n",
    "            metrics_data.append({\n",
    "                \"model\": \"BASELINE (GPT-4)\",\n",
    "                \"tested_node\": \"all\",\n",
    "                \"metric_node\": node_name,\n",
    "                \"post_id\": post_id,\n",
    "                \"execution_time\": node_metrics[\"execution_time\"],\n",
    "                \"prompt_tokens\": node_metrics[\"prompt_tokens\"],\n",
    "                \"completion_tokens\": node_metrics[\"completion_tokens\"],\n",
    "                \"total_tokens\": node_metrics[\"total_tokens\"],\n",
    "            })\n",
    "\n",
    "# Test metrics\n",
    "for test_model in TEST_MODELS:\n",
    "    for node in nodes:\n",
    "        for post_id, result in test_results.get(test_model, {}).get(node, {}).items():\n",
    "            if result.get(\"success\") and \"metrics\" in result:\n",
    "                for node_name, node_metrics in result[\"metrics\"].items():\n",
    "                    metrics_data.append({\n",
    "                        \"model\": test_model,\n",
    "                        \"tested_node\": node,\n",
    "                        \"metric_node\": node_name,\n",
    "                        \"post_id\": post_id,\n",
    "                        \"execution_time\": node_metrics[\"execution_time\"],\n",
    "                        \"prompt_tokens\": node_metrics[\"prompt_tokens\"],\n",
    "                        \"completion_tokens\": node_metrics[\"completion_tokens\"],\n",
    "                        \"total_tokens\": node_metrics[\"total_tokens\"],\n",
    "                    })\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Execution Time (seconds) by Model and Node:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_node</th>\n",
       "      <th>analyst</th>\n",
       "      <th>entry</th>\n",
       "      <th>planner</th>\n",
       "      <th>researcher</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>tested_node</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BASELINE (GPT-4)</th>\n",
       "      <th>all</th>\n",
       "      <td>4.86</td>\n",
       "      <td>7.88</td>\n",
       "      <td>13.15</td>\n",
       "      <td>11.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">llama3.1:8b</th>\n",
       "      <th>analyst</th>\n",
       "      <td>56.32</td>\n",
       "      <td>1.47</td>\n",
       "      <td>11.91</td>\n",
       "      <td>6.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <td>5.58</td>\n",
       "      <td>25.30</td>\n",
       "      <td>11.73</td>\n",
       "      <td>8.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>planner</th>\n",
       "      <td>5.18</td>\n",
       "      <td>1.60</td>\n",
       "      <td>135.17</td>\n",
       "      <td>9.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>researcher</th>\n",
       "      <td>5.64</td>\n",
       "      <td>1.72</td>\n",
       "      <td>9.56</td>\n",
       "      <td>38.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">qwen2.5:1.5b</th>\n",
       "      <th>analyst</th>\n",
       "      <td>11.63</td>\n",
       "      <td>1.74</td>\n",
       "      <td>16.68</td>\n",
       "      <td>12.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <td>4.42</td>\n",
       "      <td>5.97</td>\n",
       "      <td>10.56</td>\n",
       "      <td>6.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>planner</th>\n",
       "      <td>4.83</td>\n",
       "      <td>1.73</td>\n",
       "      <td>22.41</td>\n",
       "      <td>21.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>researcher</th>\n",
       "      <td>4.23</td>\n",
       "      <td>1.45</td>\n",
       "      <td>12.23</td>\n",
       "      <td>17.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "metric_node                   analyst  entry  planner  researcher\n",
       "model            tested_node                                     \n",
       "BASELINE (GPT-4) all             4.86   7.88    13.15       11.96\n",
       "llama3.1:8b      analyst        56.32   1.47    11.91        6.86\n",
       "                 entry           5.58  25.30    11.73        8.28\n",
       "                 planner         5.18   1.60   135.17        9.03\n",
       "                 researcher      5.64   1.72     9.56       38.75\n",
       "qwen2.5:1.5b     analyst        11.63   1.74    16.68       12.13\n",
       "                 entry           4.42   5.97    10.56        6.43\n",
       "                 planner         4.83   1.73    22.41       21.43\n",
       "                 researcher      4.23   1.45    12.23       17.31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Average execution time by model and node\n",
    "if not df_metrics.empty:\n",
    "    print(\"Average Execution Time (seconds) by Model and Node:\\n\")\n",
    "    pivot_time = df_metrics.pivot_table(\n",
    "        values=\"execution_time\",\n",
    "        index=[\"model\", \"tested_node\"],\n",
    "        columns=\"metric_node\",\n",
    "        aggfunc=\"mean\"\n",
    "    ).round(2)\n",
    "    display(pivot_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "1. SUCCESS RATE BY MODEL AND NODE:\n",
      "\n",
      "model         node      \n",
      "llama3.1:8b   analyst       100.0\n",
      "              entry         100.0\n",
      "              planner       100.0\n",
      "              researcher    100.0\n",
      "qwen2.5:1.5b  analyst       100.0\n",
      "              entry         100.0\n",
      "              planner       100.0\n",
      "              researcher    100.0\n",
      "\n",
      "2. RELEVANCE MATCH WITH BASELINE:\n",
      "\n",
      "model         node      \n",
      "llama3.1:8b   analyst       100.0\n",
      "              entry          66.7\n",
      "              planner       100.0\n",
      "              researcher    100.0\n",
      "qwen2.5:1.5b  analyst       100.0\n",
      "              entry         100.0\n",
      "              planner       100.0\n",
      "              researcher    100.0\n",
      "\n",
      "3. AVERAGE SCORE DIFFERENCE FROM BASELINE:\n",
      "\n",
      "model         node      \n",
      "llama3.1:8b   analyst       0.00\n",
      "              entry        -0.30\n",
      "              planner      -0.05\n",
      "              researcher   -0.05\n",
      "qwen2.5:1.5b  analyst      -0.25\n",
      "              entry        -0.05\n",
      "              planner      -0.10\n",
      "              researcher    0.05\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Success rate\n",
    "print(\"\\n1. SUCCESS RATE BY MODEL AND NODE:\\n\")\n",
    "if not df_comparison.empty:\n",
    "    success_rate = df_comparison.groupby([\"model\", \"node\"])[\"success\"].mean() * 100\n",
    "    print(success_rate.round(1).to_string())\n",
    "\n",
    "# Relevance match rate\n",
    "print(\"\\n2. RELEVANCE MATCH WITH BASELINE:\\n\")\n",
    "if not df_comparison.empty:\n",
    "    relevance_match = df_comparison[df_comparison[\"success\"]].groupby([\"model\", \"node\"])[\"relevance_match\"].mean() * 100\n",
    "    print(relevance_match.round(1).to_string())\n",
    "\n",
    "# Score differences\n",
    "print(\"\\n3. AVERAGE SCORE DIFFERENCE FROM BASELINE:\\n\")\n",
    "if not df_comparison.empty and \"score_diff\" in df_comparison.columns:\n",
    "    score_diff = df_comparison[df_comparison[\"score_diff\"].notna()].groupby([\"model\", \"node\"])[\"score_diff\"].mean()\n",
    "    if not score_diff.empty:\n",
    "        print(score_diff.round(2).to_string())\n",
    "    else:\n",
    "        print(\"No comparable scores available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DETAILED RESPONSE COMPARISON\n",
      "============================================================\n",
      "\n",
      "--- Post: irrelevant_1 ---\n",
      "Text: Bom dia!\n",
      "\n",
      "BASELINE (GPT-4):\n",
      "  Relevant: False\n",
      "  Reasoning: Apenas uma saudação, sem afirmações factuais verificáveis...\n",
      "\n",
      "llama3.1:8b:\n",
      "  [entry] relevant=False ✓, score=N/A\n",
      "  [planner] relevant=False ✓, score=N/A\n",
      "  [researcher] relevant=False ✓, score=N/A\n",
      "  [analyst] relevant=False ✓, score=N/A\n",
      "\n",
      "qwen2.5:1.5b:\n",
      "  [entry] relevant=False ✓, score=N/A\n",
      "  [planner] relevant=False ✓, score=N/A\n",
      "  [researcher] relevant=False ✓, score=N/A\n",
      "  [analyst] relevant=False ✓, score=N/A\n",
      "\n",
      "--- Post: fake_news_1 ---\n",
      "Text: Cloroquina cura COVID\n",
      "\n",
      "BASELINE (GPT-4):\n",
      "  Relevant: True\n",
      "  Reasoning: Contém afirmação verificável sobre eficácia médica...\n",
      "  Score: 1.0\n",
      "  Justification: A afirmação de que a cloroquina cura COVID-19 é falsa. As pesquisas indicam que a eficácia da cloroquina no tratamento da COVID-19 é insuficiente e a ...\n",
      "\n",
      "llama3.1:8b:\n",
      "  [entry] relevant=True ✓, score=0.7\n",
      "  [planner] relevant=True ✓, score=0.9\n",
      "  [researcher] relevant=True ✓, score=1.0\n",
      "  [analyst] relevant=True ✓, score=1.0\n",
      "\n",
      "qwen2.5:1.5b:\n",
      "  [entry] relevant=True ✓, score=0.9\n",
      "  [planner] relevant=True ✓, score=0.9\n",
      "  [researcher] relevant=True ✓, score=1.0\n",
      "  [analyst] relevant=True ✓, score=0.5\n",
      "\n",
      "--- Post: political_1 ---\n",
      "Text: Gastar dinheiro público com campanhas de incentivo ao aborto, ideologia de gêner...\n",
      "\n",
      "BASELINE (GPT-4):\n",
      "  Relevant: True\n",
      "  Reasoning: O post faz uma afirmação sobre políticas públicas e uso de dinheiro público, que são verificáveis....\n",
      "  Score: 0.9\n",
      "  Justification: As pesquisas realizadas não encontraram evidências de que o dinheiro público esteja sendo gasto com campanhas de incentivo ao aborto, ideologia de gên...\n",
      "\n",
      "llama3.1:8b:\n",
      "  [entry] relevant=False ✗, score=N/A\n",
      "  [planner] relevant=True ✓, score=0.9\n",
      "  [researcher] relevant=True ✓, score=0.8\n",
      "  [analyst] relevant=True ✓, score=0.9\n",
      "\n",
      "qwen2.5:1.5b:\n",
      "  [entry] relevant=True ✓, score=0.9\n",
      "  [planner] relevant=True ✓, score=0.8\n",
      "  [researcher] relevant=True ✓, score=1.0\n",
      "  [analyst] relevant=True ✓, score=0.9\n"
     ]
    }
   ],
   "source": [
    "# Detailed comparison for each post\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED RESPONSE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for post_data in TEST_POSTS:\n",
    "    post_id = post_data[\"id\"]\n",
    "    print(f\"\\n--- Post: {post_id} ---\")\n",
    "    print(f\"Text: {post_data['text'][:80]}...\" if len(post_data['text']) > 80 else f\"Text: {post_data['text']}\")\n",
    "    \n",
    "    baseline = baseline_results.get(post_id, {})\n",
    "    if baseline.get(\"success\"):\n",
    "        print(f\"\\nBASELINE (GPT-4):\")\n",
    "        print(f\"  Relevant: {baseline['relevance_analysis']['relevant']}\")\n",
    "        print(f\"  Reasoning: {baseline['relevance_analysis'].get('reasoning', '')[:150]}...\")\n",
    "        if baseline['relevance_analysis']['relevant']:\n",
    "            print(f\"  Score: {baseline['response']['score']}\")\n",
    "            print(f\"  Justification: {baseline['response'].get('justification', '')[:150]}...\")\n",
    "    \n",
    "    for test_model in TEST_MODELS:\n",
    "        print(f\"\\n{test_model}:\")\n",
    "        for node in nodes:\n",
    "            test = test_results.get(test_model, {}).get(node, {}).get(post_id, {})\n",
    "            if test.get(\"success\"):\n",
    "                rel = test['relevance_analysis']['relevant']\n",
    "                score = test.get('response', {}).get('score', 'N/A') if rel else 'N/A'\n",
    "                baseline_match = \"✓\" if rel == baseline.get('relevance_analysis', {}).get('relevant') else \"✗\"\n",
    "                print(f\"  [{node}] relevant={rel} {baseline_match}, score={score}\")\n",
    "            else:\n",
    "                print(f\"  [{node}] FAILED: {test.get('error', 'Unknown')[:30]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to evaluation_results.json\n",
      "Comparison data saved to evaluation_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Save all results to JSON for further analysis\n",
    "evaluation_output = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"baseline_model\": BASELINE_MODEL,\n",
    "    \"test_models\": TEST_MODELS,\n",
    "    \"test_posts\": TEST_POSTS,\n",
    "    \"baseline_results\": baseline_results,\n",
    "    \"test_results\": test_results,\n",
    "}\n",
    "\n",
    "with open(\"evaluation_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation_output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Results saved to evaluation_results.json\")\n",
    "\n",
    "# Save comparison DataFrame\n",
    "if not df_comparison.empty:\n",
    "    df_comparison.to_csv(\"evaluation_comparison.csv\", index=False)\n",
    "    print(\"Comparison data saved to evaluation_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
