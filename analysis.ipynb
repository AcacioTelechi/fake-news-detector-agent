{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Analysis - Batch Processing\n",
    "\n",
    "This notebook processes posts from CSV files in the `input/` folder and saves results to `output/`.\n",
    "\n",
    "**Model Configuration:**\n",
    "- Entry: qwen2.5:1.5b\n",
    "- Planner: llama3.1:8b\n",
    "- Researcher: llama3.1:8b\n",
    "- Analyst: llama3.1:8b\n",
    "\n",
    "**Features:**\n",
    "- Parallel processing with configurable workers\n",
    "- Incremental saving after each batch completes\n",
    "- Resume capability if interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, Any, List\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "import threading\n",
    "\n",
    "from src.graphs.v1 import graph\n",
    "from src.models.context import ModelsRegistry, ModelConfig\n",
    "from tavily import TavilyClient\n",
    "from src.utils.observability import (\n",
    "    UsageMetadataCallbackHandler,\n",
    "    set_callback_handler,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "Path(\"output\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models configuration: {'entry': 'qwen2.5:1.5b', 'planner': 'llama3.1:8b', 'researcher': 'llama3.1:8b', 'analyst': 'llama3.1:8b'}\n",
      "Max workers: 4\n",
      "Batch size (save every N): 10\n",
      "Output files:\n",
      "  - CSV: output/analysis_results_20260201_134524.csv\n",
      "  - JSONL: output/analysis_results_20260201_134524.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Ollama configuration\n",
    "OLLAMA_CONFIG = {\n",
    "    \"model_provider\": \"ollama\",\n",
    "    \"base_url\": \"http://localhost:11434\",\n",
    "}\n",
    "\n",
    "# Model configuration for each node\n",
    "MODELS_CONFIG = {\n",
    "    \"entry\": \"qwen2.5:1.5b\",\n",
    "    \"planner\": \"llama3.1:8b\",\n",
    "    \"researcher\": \"llama3.1:8b\",\n",
    "    \"analyst\": \"llama3.1:8b\",\n",
    "}\n",
    "\n",
    "# Parallel processing settings\n",
    "MAX_WORKERS = 4  # Adjust based on your system resources\n",
    "BATCH_SIZE = 10  # Save results after every N completed items\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "# Output file prefix (timestamp will be added)\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_CSV = f\"output/analysis_results_{RUN_ID}.csv\"\n",
    "OUTPUT_JSON = f\"output/analysis_results_{RUN_ID}.jsonl\"  # JSON Lines format for incremental writes\n",
    "\n",
    "print(f\"Models configuration: {MODELS_CONFIG}\")\n",
    "print(f\"Max workers: {MAX_WORKERS}\")\n",
    "print(f\"Batch size (save every N): {BATCH_SIZE}\")\n",
    "print(f\"Output files:\")\n",
    "print(f\"  - CSV: {OUTPUT_CSV}\")\n",
    "print(f\"  - JSONL: {OUTPUT_JSON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CSV file(s) in input folder:\n",
      "  - input\\data-1769622510591.csv\n",
      "  Loaded 1000 rows from data-1769622510591.csv\n",
      "\n",
      "Total posts to analyze: 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_mention</th>\n",
       "      <th>full_text</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00006b826eb2064cd4c069df5e9bebac</td>\n",
       "      <td>Gente pelo amor de Deus Boulos n√£o , miseric√≥r...</td>\n",
       "      <td>data-1769622510591.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00014e37ef658664f02ab02c991d45f7</td>\n",
       "      <td>Estou com Boulos 50 üôè</td>\n",
       "      <td>data-1769622510591.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00018e4f326728431c1c97b775f31b67</td>\n",
       "      <td>Qual √© sua opini√£o sobre isso? üëÄü§î . J√° que o D...</td>\n",
       "      <td>data-1769622510591.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001c9de9cf636d2b5faa31eb88a87cd</td>\n",
       "      <td>Um carro da funer√°ria acaba de entrar no hospi...</td>\n",
       "      <td>data-1769622510591.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001df78f4f1323c23cab956a749ac62</td>\n",
       "      <td>Estad√£o Conte√∫doi Estad√£o Conte√∫do https://ist...</td>\n",
       "      <td>data-1769622510591.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id_mention  \\\n",
       "0  00006b826eb2064cd4c069df5e9bebac   \n",
       "1  00014e37ef658664f02ab02c991d45f7   \n",
       "2  00018e4f326728431c1c97b775f31b67   \n",
       "3  0001c9de9cf636d2b5faa31eb88a87cd   \n",
       "4  0001df78f4f1323c23cab956a749ac62   \n",
       "\n",
       "                                           full_text             source_file  \n",
       "0  Gente pelo amor de Deus Boulos n√£o , miseric√≥r...  data-1769622510591.csv  \n",
       "1                              Estou com Boulos 50 üôè  data-1769622510591.csv  \n",
       "2  Qual √© sua opini√£o sobre isso? üëÄü§î . J√° que o D...  data-1769622510591.csv  \n",
       "3  Um carro da funer√°ria acaba de entrar no hospi...  data-1769622510591.csv  \n",
       "4  Estad√£o Conte√∫doi Estad√£o Conte√∫do https://ist...  data-1769622510591.csv  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all CSV files in input folder\n",
    "input_files = glob.glob(\"input/*.csv\")\n",
    "print(f\"Found {len(input_files)} CSV file(s) in input folder:\")\n",
    "for f in input_files:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# Load and concatenate all CSV files\n",
    "dfs = []\n",
    "for file_path in input_files:\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"source_file\"] = os.path.basename(file_path)\n",
    "    dfs.append(df)\n",
    "    print(f\"  Loaded {len(df)} rows from {os.path.basename(file_path)}\")\n",
    "\n",
    "df_input = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "print(f\"\\nTotal posts to analyze: {len(df_input)}\")\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Tavily client (shared across workers)\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "def create_models_registry() -> ModelsRegistry:\n",
    "    \"\"\"Create a models registry with the configured models.\"\"\"\n",
    "    return ModelsRegistry(\n",
    "        entry=ModelConfig(\n",
    "            model=MODELS_CONFIG[\"entry\"],\n",
    "            temperature=TEMPERATURE,\n",
    "            **OLLAMA_CONFIG,\n",
    "        ),\n",
    "        planner=ModelConfig(\n",
    "            model=MODELS_CONFIG[\"planner\"],\n",
    "            temperature=TEMPERATURE,\n",
    "            **OLLAMA_CONFIG,\n",
    "        ),\n",
    "        researcher=ModelConfig(\n",
    "            model=MODELS_CONFIG[\"researcher\"],\n",
    "            temperature=TEMPERATURE,\n",
    "            **OLLAMA_CONFIG,\n",
    "        ),\n",
    "        analyst=ModelConfig(\n",
    "            model=MODELS_CONFIG[\"analyst\"],\n",
    "            temperature=TEMPERATURE,\n",
    "            **OLLAMA_CONFIG,\n",
    "        ),\n",
    "        default_temperature=TEMPERATURE,\n",
    "    )\n",
    "\n",
    "print(\"Components initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Save Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save functions initialized!\n"
     ]
    }
   ],
   "source": [
    "# Thread-safe file writing\n",
    "file_lock = threading.Lock()\n",
    "\n",
    "def save_result_jsonl(result: Dict[str, Any], filepath: str):\n",
    "    \"\"\"Append a single result to JSONL file (thread-safe).\"\"\"\n",
    "    with file_lock:\n",
    "        with open(filepath, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def save_batch_csv(results: List[Dict[str, Any]], filepath: str, is_first_batch: bool = False):\n",
    "    \"\"\"Save a batch of results to CSV (append mode).\"\"\"\n",
    "    csv_columns = [\n",
    "        \"id_mention\", \"full_text\", \"success\", \"error\",\n",
    "        \"relevant\", \"relevance_reasoning\",\n",
    "        \"score\", \"justification\", \"plan\",\n",
    "        \"processing_time_s\", \"timestamp\",\n",
    "    ]\n",
    "    \n",
    "    df_batch = pd.DataFrame(results)\n",
    "    # Select only columns that exist\n",
    "    cols_to_save = [c for c in csv_columns if c in df_batch.columns]\n",
    "    df_batch = df_batch[cols_to_save]\n",
    "    \n",
    "    with file_lock:\n",
    "        # Write header only for first batch\n",
    "        df_batch.to_csv(\n",
    "            filepath, \n",
    "            mode=\"w\" if is_first_batch else \"a\",\n",
    "            header=is_first_batch,\n",
    "            index=False, \n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "\n",
    "def load_existing_results(jsonl_path: str) -> set:\n",
    "    \"\"\"Load IDs of already processed posts (for resume capability).\"\"\"\n",
    "    processed_ids = set()\n",
    "    if os.path.exists(jsonl_path):\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    result = json.loads(line.strip())\n",
    "                    if \"id_mention\" in result:\n",
    "                        processed_ids.add(result[\"id_mention\"])\n",
    "                except:\n",
    "                    pass\n",
    "    return processed_ids\n",
    "\n",
    "\n",
    "print(\"Save functions initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_post(post_id: str, post_text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze a single post and return the result.\n",
    "    \n",
    "    Args:\n",
    "        post_id: Unique identifier for the post\n",
    "        post_text: The text content to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    # Create fresh instances for thread safety\n",
    "    callback = UsageMetadataCallbackHandler()\n",
    "    set_callback_handler(callback)\n",
    "    models_registry = create_models_registry()\n",
    "    \n",
    "    config = {\n",
    "        \"configurable\": {\"thread_id\": f\"analysis_{post_id}\"},\n",
    "        \"callbacks\": [callback],\n",
    "    }\n",
    "    runtime_context = {\"models_registry\": models_registry, \"tavily\": tavily}\n",
    "    \n",
    "    initial_state = {\n",
    "        \"post\": post_text,\n",
    "        \"max_revisions\": 3,\n",
    "    }\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        resp = graph.invoke(initial_state, context=runtime_context, config=config)\n",
    "        \n",
    "        # Build result\n",
    "        result = {\n",
    "            \"id_mention\": post_id,\n",
    "            \"full_text\": post_text,\n",
    "            \"success\": True,\n",
    "            \"error\": None,\n",
    "            \"timestamp\": start_time.isoformat(),\n",
    "            \"processing_time_s\": (datetime.now() - start_time).total_seconds(),\n",
    "            \"models_config\": MODELS_CONFIG,\n",
    "        }\n",
    "        \n",
    "        # Relevance analysis\n",
    "        rel_analysis = resp[\"relevance_analysis\"]\n",
    "        if hasattr(rel_analysis, \"model_dump\"):\n",
    "            rel_analysis = rel_analysis.model_dump()\n",
    "        result[\"relevant\"] = rel_analysis.get(\"relevant\", False)\n",
    "        result[\"relevance_reasoning\"] = rel_analysis.get(\"reasoning\", \"\")\n",
    "        \n",
    "        # If relevant, include full analysis\n",
    "        if result[\"relevant\"]:\n",
    "            result[\"plan\"] = resp.get(\"plan\", \"\")\n",
    "            \n",
    "            response = resp.get(\"response\")\n",
    "            if response:\n",
    "                if hasattr(response, \"model_dump\"):\n",
    "                    response = response.model_dump()\n",
    "                result[\"score\"] = response.get(\"score\")\n",
    "                result[\"justification\"] = response.get(\"justification\", \"\")\n",
    "            \n",
    "            # Include references if available\n",
    "            result[\"references\"] = resp.get(\"references\", [])\n",
    "        else:\n",
    "            result[\"plan\"] = None\n",
    "            result[\"score\"] = None\n",
    "            result[\"justification\"] = None\n",
    "            result[\"references\"] = []\n",
    "        \n",
    "        # Metrics\n",
    "        metrics = {}\n",
    "        for node_name, node_metrics in resp.get(\"metrics\", {}).items():\n",
    "            if hasattr(node_metrics, \"model_dump\"):\n",
    "                metrics[node_name] = node_metrics.model_dump()\n",
    "            else:\n",
    "                metrics[node_name] = node_metrics\n",
    "        result[\"metrics\"] = metrics\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"id_mention\": post_id,\n",
    "            \"full_text\": post_text,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": start_time.isoformat(),\n",
    "            \"processing_time_s\": (datetime.now() - start_time).total_seconds(),\n",
    "            \"models_config\": MODELS_CONFIG,\n",
    "            \"relevant\": None,\n",
    "            \"relevance_reasoning\": None,\n",
    "            \"plan\": None,\n",
    "            \"score\": None,\n",
    "            \"justification\": None,\n",
    "            \"references\": [],\n",
    "            \"metrics\": {},\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Parallel Analysis with Incremental Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_analysis_incremental(\n",
    "    df: pd.DataFrame,\n",
    "    output_csv: str,\n",
    "    output_jsonl: str,\n",
    "    max_workers: int = MAX_WORKERS,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    resume: bool = True,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run analysis on all posts in parallel with incremental saving.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with id_mention and full_text columns\n",
    "        output_csv: Path to output CSV file\n",
    "        output_jsonl: Path to output JSONL file\n",
    "        max_workers: Maximum number of parallel workers\n",
    "        batch_size: Save to CSV after every N completed items\n",
    "        resume: If True, skip already processed posts\n",
    "        \n",
    "    Returns:\n",
    "        List of all analysis results\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    batch_results = []\n",
    "    batches_saved = 0\n",
    "    \n",
    "    # Check for existing results (resume capability)\n",
    "    processed_ids = set()\n",
    "    if resume and os.path.exists(output_jsonl):\n",
    "        processed_ids = load_existing_results(output_jsonl)\n",
    "        print(f\"Found {len(processed_ids)} already processed posts, resuming...\")\n",
    "    \n",
    "    # Filter out already processed\n",
    "    df_to_process = df[~df[\"id_mention\"].isin(processed_ids)].copy()\n",
    "    total = len(df_to_process)\n",
    "    \n",
    "    if total == 0:\n",
    "        print(\"All posts already processed!\")\n",
    "        return all_results\n",
    "    \n",
    "    print(f\"Starting parallel analysis of {total} posts with {max_workers} workers...\")\n",
    "    print(f\"Saving every {batch_size} completed items\")\n",
    "    print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Initialize CSV with header if starting fresh\n",
    "    is_first_batch = not os.path.exists(output_csv) or len(processed_ids) == 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_post = {\n",
    "            executor.submit(analyze_post, row[\"id_mention\"], row[\"full_text\"]): row[\"id_mention\"]\n",
    "            for _, row in df_to_process.iterrows()\n",
    "        }\n",
    "        \n",
    "        # Process completed tasks with progress bar\n",
    "        with tqdm(total=total, desc=\"Analyzing posts\") as pbar:\n",
    "            for future in as_completed(future_to_post):\n",
    "                post_id = future_to_post[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    all_results.append(result)\n",
    "                    batch_results.append(result)\n",
    "                    \n",
    "                    # Save to JSONL immediately (one line per result)\n",
    "                    save_result_jsonl(result, output_jsonl)\n",
    "                    \n",
    "                    # Update progress bar with status\n",
    "                    status = \"‚úì\" if result[\"success\"] else \"‚úó\"\n",
    "                    relevant = \"R\" if result.get(\"relevant\") else \"-\"\n",
    "                    pbar.set_postfix_str(f\"Last: {post_id[:8]}... [{status}{relevant}]\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_result = {\n",
    "                        \"id_mention\": post_id,\n",
    "                        \"success\": False,\n",
    "                        \"error\": f\"Future error: {str(e)}\",\n",
    "                        \"timestamp\": datetime.now().isoformat(),\n",
    "                    }\n",
    "                    all_results.append(error_result)\n",
    "                    batch_results.append(error_result)\n",
    "                    save_result_jsonl(error_result, output_jsonl)\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Save batch to CSV when batch_size reached\n",
    "                if len(batch_results) >= batch_size:\n",
    "                    save_batch_csv(batch_results, output_csv, is_first_batch and batches_saved == 0)\n",
    "                    batches_saved += 1\n",
    "                    pbar.set_description(f\"Analyzing posts (saved batch {batches_saved})\")\n",
    "                    batch_results = []  # Reset batch\n",
    "        \n",
    "        # Save any remaining results\n",
    "        if batch_results:\n",
    "            save_batch_csv(batch_results, output_csv, is_first_batch and batches_saved == 0)\n",
    "            batches_saved += 1\n",
    "    \n",
    "    print(f\"\\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Total batches saved: {batches_saved}\")\n",
    "    \n",
    "    # Summary\n",
    "    successful = sum(1 for r in all_results if r.get(\"success\"))\n",
    "    relevant = sum(1 for r in all_results if r.get(\"relevant\"))\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  - Total processed this run: {len(all_results)}\")\n",
    "    print(f\"  - Successful: {successful} ({100*successful/len(all_results):.1f}%)\")\n",
    "    print(f\"  - Failed: {len(all_results) - successful}\")\n",
    "    print(f\"  - Relevant posts: {relevant}\")\n",
    "    print(f\"\\nResults saved to:\")\n",
    "    print(f\"  - {output_csv}\")\n",
    "    print(f\"  - {output_jsonl}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel analysis of 1000 posts with 4 workers...\n",
      "Saving every 10 completed items\n",
      "Start time: 2026-02-01 13:45:24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27464ebd644c4265b4e9d174a8c4cc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing posts:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the analysis with incremental saving\n",
    "results = run_parallel_analysis_incremental(\n",
    "    df_input,\n",
    "    output_csv=OUTPUT_CSV,\n",
    "    output_jsonl=OUTPUT_JSON,\n",
    "    max_workers=MAX_WORKERS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    resume=True,  # Set to False to start fresh\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Analyze All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results loaded: 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_mention</th>\n",
       "      <th>full_text</th>\n",
       "      <th>success</th>\n",
       "      <th>error</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>processing_time_s</th>\n",
       "      <th>models_config</th>\n",
       "      <th>relevant</th>\n",
       "      <th>relevance_reasoning</th>\n",
       "      <th>plan</th>\n",
       "      <th>score</th>\n",
       "      <th>justification</th>\n",
       "      <th>references</th>\n",
       "      <th>metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00018e4f326728431c1c97b775f31b67</td>\n",
       "      <td>Qual √© sua opini√£o sobre isso? üëÄü§î . J√° que o D...</td>\n",
       "      <td>False</td>\n",
       "      <td>Initializing ChatOllama requires the langchain...</td>\n",
       "      <td>2026-01-31T14:30:28.044473</td>\n",
       "      <td>0.074004</td>\n",
       "      <td>{'entry': 'qwen2.5:1.5b', 'planner': 'llama3.1...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001c9de9cf636d2b5faa31eb88a87cd</td>\n",
       "      <td>Um carro da funer√°ria acaba de entrar no hospi...</td>\n",
       "      <td>False</td>\n",
       "      <td>Initializing ChatOllama requires the langchain...</td>\n",
       "      <td>2026-01-31T14:30:28.045491</td>\n",
       "      <td>0.075088</td>\n",
       "      <td>{'entry': 'qwen2.5:1.5b', 'planner': 'llama3.1...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00014e37ef658664f02ab02c991d45f7</td>\n",
       "      <td>Estou com Boulos 50 üôè</td>\n",
       "      <td>False</td>\n",
       "      <td>Initializing ChatOllama requires the langchain...</td>\n",
       "      <td>2026-01-31T14:30:28.043418</td>\n",
       "      <td>0.073206</td>\n",
       "      <td>{'entry': 'qwen2.5:1.5b', 'planner': 'llama3.1...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00006b826eb2064cd4c069df5e9bebac</td>\n",
       "      <td>Gente pelo amor de Deus Boulos n√£o , miseric√≥r...</td>\n",
       "      <td>False</td>\n",
       "      <td>Initializing ChatOllama requires the langchain...</td>\n",
       "      <td>2026-01-31T14:30:28.042142</td>\n",
       "      <td>0.072423</td>\n",
       "      <td>{'entry': 'qwen2.5:1.5b', 'planner': 'llama3.1...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001df78f4f1323c23cab956a749ac62</td>\n",
       "      <td>Estad√£o Conte√∫doi Estad√£o Conte√∫do https://ist...</td>\n",
       "      <td>False</td>\n",
       "      <td>Initializing ChatOllama requires the langchain...</td>\n",
       "      <td>2026-01-31T14:30:28.114637</td>\n",
       "      <td>0.013544</td>\n",
       "      <td>{'entry': 'qwen2.5:1.5b', 'planner': 'llama3.1...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id_mention  \\\n",
       "0  00018e4f326728431c1c97b775f31b67   \n",
       "1  0001c9de9cf636d2b5faa31eb88a87cd   \n",
       "2  00014e37ef658664f02ab02c991d45f7   \n",
       "3  00006b826eb2064cd4c069df5e9bebac   \n",
       "4  0001df78f4f1323c23cab956a749ac62   \n",
       "\n",
       "                                           full_text  success  \\\n",
       "0  Qual √© sua opini√£o sobre isso? üëÄü§î . J√° que o D...    False   \n",
       "1  Um carro da funer√°ria acaba de entrar no hospi...    False   \n",
       "2                              Estou com Boulos 50 üôè    False   \n",
       "3  Gente pelo amor de Deus Boulos n√£o , miseric√≥r...    False   \n",
       "4  Estad√£o Conte√∫doi Estad√£o Conte√∫do https://ist...    False   \n",
       "\n",
       "                                               error  \\\n",
       "0  Initializing ChatOllama requires the langchain...   \n",
       "1  Initializing ChatOllama requires the langchain...   \n",
       "2  Initializing ChatOllama requires the langchain...   \n",
       "3  Initializing ChatOllama requires the langchain...   \n",
       "4  Initializing ChatOllama requires the langchain...   \n",
       "\n",
       "                    timestamp  processing_time_s  \\\n",
       "0  2026-01-31T14:30:28.044473           0.074004   \n",
       "1  2026-01-31T14:30:28.045491           0.075088   \n",
       "2  2026-01-31T14:30:28.043418           0.073206   \n",
       "3  2026-01-31T14:30:28.042142           0.072423   \n",
       "4  2026-01-31T14:30:28.114637           0.013544   \n",
       "\n",
       "                                       models_config relevant  \\\n",
       "0  {'entry': 'qwen2.5:1.5b', 'planner': 'llama3.1...     None   \n",
       "1  {'entry': 'qwen2.5:1.5b', 'planner': 'llama3.1...     None   \n",
       "2  {'entry': 'qwen2.5:1.5b', 'planner': 'llama3.1...     None   \n",
       "3  {'entry': 'qwen2.5:1.5b', 'planner': 'llama3.1...     None   \n",
       "4  {'entry': 'qwen2.5:1.5b', 'planner': 'llama3.1...     None   \n",
       "\n",
       "  relevance_reasoning  plan score justification references metrics  \n",
       "0                None  None  None          None         []      {}  \n",
       "1                None  None  None          None         []      {}  \n",
       "2                None  None  None          None         []      {}  \n",
       "3                None  None  None          None         []      {}  \n",
       "4                None  None  None          None         []      {}  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all results from JSONL (includes any previous runs if resumed)\n",
    "all_results = []\n",
    "with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        all_results.append(json.loads(line.strip()))\n",
    "\n",
    "df_results = pd.DataFrame(all_results)\n",
    "print(f\"Total results loaded: {len(df_results)}\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANALYSIS SUMMARY\n",
      "============================================================\n",
      "\n",
      "1. PROCESSING STATISTICS:\n",
      "   Total posts: 1000\n",
      "   Successful: 0 (0.0%)\n",
      "   Failed: 1000\n",
      "\n",
      "2. RELEVANCE DISTRIBUTION:\n",
      "\n",
      "3. SCORE DISTRIBUTION (relevant posts only):\n",
      "   No scores available\n",
      "\n",
      "4. PROCESSING TIME:\n",
      "   Mean: 0.01s\n",
      "   Min: 0.00s\n",
      "   Max: 0.08s\n",
      "   Total: 10.87s (0.2 min)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Success rate\n",
    "print(f\"\\n1. PROCESSING STATISTICS:\")\n",
    "print(f\"   Total posts: {len(df_results)}\")\n",
    "print(f\"   Successful: {df_results['success'].sum()} ({100*df_results['success'].mean():.1f}%)\")\n",
    "print(f\"   Failed: {(~df_results['success']).sum()}\")\n",
    "\n",
    "# Relevance distribution\n",
    "print(f\"\\n2. RELEVANCE DISTRIBUTION:\")\n",
    "relevant_df = df_results[df_results['success']]\n",
    "if len(relevant_df) > 0:\n",
    "    relevant_count = relevant_df['relevant'].sum()\n",
    "    not_relevant_count = len(relevant_df) - relevant_count\n",
    "    print(f\"   Relevant: {relevant_count} ({100*relevant_count/len(relevant_df):.1f}%)\")\n",
    "    print(f\"   Not relevant: {not_relevant_count} ({100*not_relevant_count/len(relevant_df):.1f}%)\")\n",
    "\n",
    "# Score distribution (for relevant posts)\n",
    "print(f\"\\n3. SCORE DISTRIBUTION (relevant posts only):\")\n",
    "scores = df_results[df_results['relevant'] == True]['score'].dropna()\n",
    "if len(scores) > 0:\n",
    "    print(f\"   Count: {len(scores)}\")\n",
    "    print(f\"   Mean: {scores.mean():.2f}\")\n",
    "    print(f\"   Std: {scores.std():.2f}\")\n",
    "    print(f\"   Min: {scores.min():.2f}\")\n",
    "    print(f\"   Max: {scores.max():.2f}\")\n",
    "    print(f\"   Median: {scores.median():.2f}\")\n",
    "else:\n",
    "    print(\"   No scores available\")\n",
    "\n",
    "# Processing time\n",
    "print(f\"\\n4. PROCESSING TIME:\")\n",
    "if 'processing_time_s' in df_results.columns:\n",
    "    times = df_results['processing_time_s'].dropna()\n",
    "    if len(times) > 0:\n",
    "        print(f\"   Mean: {times.mean():.2f}s\")\n",
    "        print(f\"   Min: {times.min():.2f}s\")\n",
    "        print(f\"   Max: {times.max():.2f}s\")\n",
    "        print(f\"   Total: {times.sum():.2f}s ({times.sum()/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAMPLE OF RELEVANT POSTS (sorted by score):\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Show sample of relevant posts with high scores\n",
    "print(\"\\nSAMPLE OF RELEVANT POSTS (sorted by score):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "relevant_posts = df_results[\n",
    "    (df_results['relevant'] == True) & \n",
    "    (df_results['score'].notna())\n",
    "].sort_values('score', ascending=False)\n",
    "\n",
    "for i, row in relevant_posts.head(10).iterrows():\n",
    "    print(f\"\\nID: {row['id_mention']}\")\n",
    "    print(f\"Score: {row['score']}\")\n",
    "    text = row['full_text'][:150] + \"...\" if len(str(row['full_text'])) > 150 else row['full_text']\n",
    "    print(f\"Text: {text}\")\n",
    "    justification = str(row['justification'])[:200] + \"...\" if len(str(row['justification'])) > 200 else row['justification']\n",
    "    print(f\"Justification: {justification}\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FAILED POSTS (1000 total):\n",
      "============================================================\n",
      "ID: 00018e4f326728431c1c97b775f31b67\n",
      "Error: Initializing ChatOllama requires the langchain-ollama package. Please install it with `pip install langchain-ollama`\n",
      "----------------------------------------\n",
      "ID: 0001c9de9cf636d2b5faa31eb88a87cd\n",
      "Error: Initializing ChatOllama requires the langchain-ollama package. Please install it with `pip install langchain-ollama`\n",
      "----------------------------------------\n",
      "ID: 00014e37ef658664f02ab02c991d45f7\n",
      "Error: Initializing ChatOllama requires the langchain-ollama package. Please install it with `pip install langchain-ollama`\n",
      "----------------------------------------\n",
      "ID: 00006b826eb2064cd4c069df5e9bebac\n",
      "Error: Initializing ChatOllama requires the langchain-ollama package. Please install it with `pip install langchain-ollama`\n",
      "----------------------------------------\n",
      "ID: 0001df78f4f1323c23cab956a749ac62\n",
      "Error: Initializing ChatOllama requires the langchain-ollama package. Please install it with `pip install langchain-ollama`\n",
      "----------------------------------------\n",
      "ID: 0001e447a041f7e616400490a9a0832e\n",
      "Error: Initializing ChatOllama requires the langchain-ollama package. Please install it with `pip install langchain-ollama`\n",
      "----------------------------------------\n",
      "ID: 0002aa7086323d8552cf9a13b308c933\n",
      "Error: Initializing ChatOllama requires the langchain-ollama package. Please install it with `pip install langchain-ollama`\n",
      "----------------------------------------\n",
      "ID: 0002de06e86090962e823c9b86b4d55c\n",
      "Error: Initializing ChatOllama requires the langchain-ollama package. Please install it with `pip install langchain-ollama`\n",
      "----------------------------------------\n",
      "ID: 0002e0badd93ce3385b6474aa8ab2287\n",
      "Error: Initializing ChatOllama requires the langchain-ollama package. Please install it with `pip install langchain-ollama`\n",
      "----------------------------------------\n",
      "ID: 0003a379095b09698a0508614a8f1d92\n",
      "Error: Initializing ChatOllama requires the langchain-ollama package. Please install it with `pip install langchain-ollama`\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show errors if any\n",
    "failed = df_results[df_results['success'] == False]\n",
    "if len(failed) > 0:\n",
    "    print(f\"\\nFAILED POSTS ({len(failed)} total):\")\n",
    "    print(\"=\"*60)\n",
    "    for i, row in failed.head(10).iterrows():\n",
    "        print(f\"ID: {row['id_mention']}\")\n",
    "        print(f\"Error: {row.get('error', 'Unknown')}\")\n",
    "        print(\"-\"*40)\n",
    "else:\n",
    "    print(\"\\nNo failed posts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Final Consolidated JSON (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated JSON saved to: output/analysis_results_20260131_143027_final.json\n"
     ]
    }
   ],
   "source": [
    "# Optionally export a single consolidated JSON file at the end\n",
    "consolidated_json = f\"output/analysis_results_{RUN_ID}_final.json\"\n",
    "with open(consolidated_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Consolidated JSON saved to: {consolidated_json}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
